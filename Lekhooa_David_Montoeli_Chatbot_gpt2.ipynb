{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GtmAadarshaOmega/Facial-Expression-Recognition-FER-/blob/main/Lekhooa_David_Montoeli_Chatbot_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "gk_uvJIPdjS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVsxJiWgIM-",
        "outputId": "f2c04cdd-3067-4607-80f2-d103a5161a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:"
      ],
      "metadata": {
        "id": "dHsmOytxi9nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Numbers and Special Characters: You can further clean the text by removing numbers and special characters that might not be relevant to the intent recognition task."
      ],
      "metadata": {
        "id": "IaVKnQ6MdtGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming or Lemmatization: Depending on your use case, you can apply stemming or lemmatization to reduce words to their base forms."
      ],
      "metadata": {
        "id": "VPUoEHNfd7Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Linguistic Flags:\n",
        "Depending on the linguistic flags, you can adapt responses in more complex ways. For example, for flag 'L' (Lexical variation - synonyms), you can replace words with synonyms using a thesaurus library like NLTK's WordNet."
      ],
      "metadata": {
        "id": "Ifj7Rf32eB8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Preprocessed Data in JSON Format: Instead of saving the preprocessed data as a CSV, you can save it in JSON format, which is more flexible for handling nested data structures if needed."
      ],
      "metadata": {
        "id": "rDUErWQkeJx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "data = pd.read_csv('/Intentrecognition.csv')  # Replace with your dataset file\n",
        "\n",
        "\n",
        "# Data Cleaning\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in filtered_words]\n",
        "    # Join words back into text\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['utterance_cleaned'] = data['utterance'].apply(clean_text)\n",
        "\n",
        "# Text Tokenization\n",
        "data['tokens'] = data['utterance_cleaned'].apply(word_tokenize)\n",
        "\n",
        "# Linguistic Flags and Response Adaptation\n",
        "def adapt_response(text, flags):\n",
        "    if 'P' in flags:\n",
        "        text = \"Thank you for your question.\"\n",
        "    if 'Q' in flags:\n",
        "        text = \"Sure thing, ask away!\"\n",
        "\n",
        "    # Synonym Replacement\n",
        "    def synonym_replacement(tokens):\n",
        "        new_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in stopwords.words('english'):\n",
        "                synonyms = wn.synsets(token)\n",
        "                if synonyms:\n",
        "                    synonym = synonyms[0].lemmas()[0].name()\n",
        "                    new_tokens.append(synonym)\n",
        "                else:\n",
        "                    new_tokens.append(token)\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "        return new_tokens\n",
        "\n",
        "    if 'L' in flags:\n",
        "        tokens = word_tokenize(text)\n",
        "        new_tokens = synonym_replacement(tokens)\n",
        "        text = ' '.join(new_tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "data['bot_response'] = data.apply(lambda row: adapt_response(row['utterance'], row['flags']), axis=1)\n",
        "\n",
        "# Save preprocessed data as JSON\n",
        "data.to_json('preprocessed_dataset.json', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "RTFZ6pIUhl-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Summary:\n",
        "\n",
        "Data Loading: The code begins by loading the preprocessed dataset in JSON format using pd.read_json.\n",
        "\n",
        "Tokenizer Definition: A tokenizer specific to the transformer-based model (e.g., GPT-2) is defined using transformers.GPT2Tokenizer.from_pretrained.\n",
        "\n",
        "Data Splitting: The dataset is split into training, validation, and testing sets using train_test_split from sklearn.model_selection.\n",
        "\n",
        "Text Encoding: An encode_text function is defined to tokenize and encode text using the transformer model's tokenizer. It handles padding, truncation, and other necessary transformations.\n",
        "\n",
        "Encoding Application: Tokenization and encoding are applied to the training, validation, and testing data by adding a new column with input IDs.\n",
        "\n",
        "Label Encoding: Intent labels are converted into numerical format using label encoding. A single LabelEncoder instance is used for both training and validation labels.\n",
        "\n",
        "Data Saving: The preprocessed data for each split (training, validation, testing) is saved in JSON format using to_json.\n",
        "\n",
        "Model Artifacts Saving: The LabelEncoder and tokenizer are saved using joblib, which is an efficient method for saving and loading Python objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "k0L6ePIvkGyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG3nkInnkg4Q",
        "outputId": "1795d017-f161-4fdb-aa5e-2ea55be9ac76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLTDZOEmBNd",
        "outputId": "a053499a-e651-46f0-e23c-b22be595b332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: tzdata, pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.1.1 tzdata-2023.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer  # Assuming you're using a transformer-based model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import h5py\n",
        "\n",
        "# Load your preprocessed dataset\n",
        "data = pd.read_json('preprocessed_dataset.json', orient='records', lines=True)\n",
        "\n",
        "# Define your model-specific tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Add a padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # You can choose any suitable token for padding\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "def encode_text(text):\n",
        "    encoding = tokenizer.encode(\n",
        "        text, add_special_tokens=True, padding='max_length', max_length=128, truncation=True, return_tensors='np')  # Use 'np' to return a NumPy array\n",
        "    return encoding[0]  # Convert to 1D array\n",
        "\n",
        "# Apply tokenization and encoding to training, validation, and testing data\n",
        "train_data['input_ids'] = train_data['utterance_cleaned'].apply(encode_text)\n",
        "valid_data['input_ids'] = valid_data['utterance_cleaned'].apply(encode_text)\n",
        "test_data['input_ids'] = test_data['utterance_cleaned'].apply(encode_text)\n",
        "\n",
        "# Define target labels (intents) for training and validation\n",
        "train_labels = train_data['intent']\n",
        "valid_labels = valid_data['intent']\n",
        "\n",
        "# Use LabelEncoder to convert labels to numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "valid_labels_encoded = label_encoder.transform(valid_labels)\n",
        "\n",
        "# Save the label encoder for future use\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Convert the Pandas DataFrames to NumPy arrays\n",
        "train_data_array = np.vstack(train_data['input_ids'].to_numpy())\n",
        "valid_data_array = np.vstack(valid_data['input_ids'].to_numpy())\n",
        "test_data_array = np.vstack(test_data['input_ids'].to_numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming train_data, valid_data, and test_data are your data arrays\n",
        "\n",
        "# Function to check if a string is numeric\n",
        "def is_numeric(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Vectorize the function to work with NumPy arrays\n",
        "is_numeric_vectorized = np.vectorize(is_numeric)\n",
        "\n",
        "# Convert elements in train_data to strings\n",
        "train_data_strings = np.array([str(item) for item in train_data])\n",
        "\n",
        "# Check if the elements in train_data are numeric\n",
        "train_numeric_mask = is_numeric_vectorized(train_data_strings)\n",
        "\n",
        "# Replace non-numeric values with '0'\n",
        "train_data_strings[~train_numeric_mask] = '0'\n",
        "\n",
        "# Convert train_data back to float32\n",
        "train_data = train_data_strings.astype(np.float32)\n",
        "\n",
        "# Convert elements in valid_data to strings\n",
        "valid_data_strings = np.array([str(item) for item in valid_data])\n",
        "\n",
        "# Check if the elements in valid_data are numeric\n",
        "valid_numeric_mask = is_numeric_vectorized(valid_data_strings)\n",
        "\n",
        "# Replace non-numeric values with '0'\n",
        "valid_data_strings[~valid_numeric_mask] = '0'\n",
        "\n",
        "# Convert valid_data back to float32\n",
        "valid_data = valid_data_strings.astype(np.float32)\n",
        "\n",
        "# Convert elements in test_data to strings\n",
        "test_data_strings = np.array([str(item) for item in test_data])\n",
        "\n",
        "# Check if the elements in test_data are numeric\n",
        "test_numeric_mask = is_numeric_vectorized(test_data_strings)\n",
        "\n",
        "# Replace non-numeric values with '0'\n",
        "test_data_strings[~test_numeric_mask] = '0'\n",
        "\n",
        "# Convert test_data back to float32\n",
        "test_data = test_data_strings.astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create an HDF5 file and save the NumPy arrays as datasets\n",
        "with h5py.File('data.h5', 'w') as hf:\n",
        "    hf.create_dataset('train_data', data=train_data)\n",
        "    hf.create_dataset('valid_data', data=valid_data)\n",
        "    hf.create_dataset('test_data', data=test_data)\n",
        "\n",
        "\n",
        "# Create an HDF5 file and save the NumPy arrays as datasets\n",
        "with h5py.File('data.h5', 'w') as hf:\n",
        "    hf.create_dataset('train_data', data=train_data)\n",
        "    hf.create_dataset('valid_data', data=valid_data)\n",
        "    hf.create_dataset('test_data', data=test_data)"
      ],
      "metadata": {
        "id": "W2KNqqX-jQ1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "uwZPqsgL34kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Summary:\n",
        "\n",
        "This code prepares and saves text data for training a machine learning model using a GPT-2 tokenizer and HDF5 file format.\n",
        "\n",
        "The key steps include:\n",
        "\n",
        "Loading a preprocessed dataset from a JSON file.\n",
        "\n",
        "Defining a GPT-2 tokenizer and adding a padding token.\n",
        "\n",
        "Splitting the dataset into training, validation, and testing sets.\n",
        "\n",
        "Tokenizing and encoding text data, returning it as NumPy arrays.\n",
        "\n",
        "Encoding target labels using LabelEncoder.\n",
        "\n",
        "Saving the label encoder for future use.\n",
        "\n",
        "Converting Pandas DataFrames to NumPy arrays.\n",
        "\n",
        "Creating an HDF5 file and saving the NumPy arrays as datasets.\n",
        "\n",
        "This code streamlines data preprocessing and storage for model training.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SfYWxM0zo4FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pk7EAcMFirH",
        "outputId": "6b5b35fa-9add-4868-dec3-10a689db5362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "import h5py"
      ],
      "metadata": {
        "id": "OCaNN0qB2YvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Open the HDF5 file in read mode\n",
        "with h5py.File('data.h5', 'r') as hf:\n",
        "    # Print the keys (top-level groups) in the HDF5 file\n",
        "    print(\"Keys: \", list(hf.keys()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laSRO2OyGUAU",
        "outputId": "114e01da-06b9-4d1d-e155-8cbf618fbefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys:  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "batch_size = 16  # You can set this to your desired batch size\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Assuming you have a DataFrame 'data' with a column 'intent' that contains intent labels\n",
        "num_classes = len(data['intent'].unique())\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IntentDataset(train_data_list, train_labels_encoded.tolist())\n",
        "valid_dataset = IntentDataset(valid_data_list, valid_labels_encoded.tolist())\n",
        "\n",
        "# Define batch size and create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "\n",
        "# Assuming valid_data is a list of dictionaries or numpy arrays\n",
        "valid_input_ids = []\n",
        "valid_labels = []\n",
        "\n",
        "for item in valid_data:\n",
        "    if isinstance(item, dict) and 'valid_input_ids' in item and 'valid_labels' in item:\n",
        "        valid_input_ids.append(item['valid_input_ids'])\n",
        "        valid_labels.append(item['valid_labels'])\n",
        "    elif isinstance(item, np.ndarray) and item.shape == (2,):  # Assuming the shape of valid_data elements is (2,)\n",
        "        valid_input_ids.append(item[0])\n",
        "        valid_labels.append(item[1])\n",
        "    else:\n",
        "        # Handle the case where valid_data element does not have the expected structure\n",
        "        print(\"Invalid element found in valid_data:\", item)\n",
        "\n",
        "# Now valid_input_ids and valid_labels contain the data if the keys are present in the dictionaries or the shape is as expected\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV using Pandas\n",
        "df = pd.read_csv('/Intentrecognition.csv', delimiter=',', encoding='utf-8-sig')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['input_ids']\n",
        "\n",
        "# Encode the non-numeric column\n",
        "df['column_name'] = label_encoder.fit_transform(df['column_name'])\n",
        "\n",
        "\n",
        "\n",
        "# Verify the shape of the loaded data (it should be 2D)\n",
        "print(test_data.shape)\n",
        "\n",
        "# Manually create test_data as a list of tuples (input_ids, labels)\n",
        "test_data = [(input_ids_1, labels_1), (input_ids_2, labels_2), ...]\n",
        "\n",
        "# Access input_ids and labels separately\n",
        "test_input_ids = [item[0] for item in test_data]\n",
        "test_labels = [item[1] for item in test_data]\n",
        "\n",
        "\n",
        "class IntentDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Check if 'input_ids' key exists in self.data[idx]\n",
        "        if 'input_ids' in self.data[idx]:\n",
        "            input_ids = torch.tensor(self.data[idx]['input_ids'], dtype=torch.long)\n",
        "        else:\n",
        "            # Handle the case where 'input_ids' key is missing or provide a default value\n",
        "            input_ids = torch.zeros(1, dtype=torch.long)  # Replace with appropriate default value or handling\n",
        "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert your data into a list of dictionaries where each dictionary contains input_ids and labels\n",
        "train_data_list = [{'train_input_ids': input_ids, 'train_labels': label} for input_ids, label in zip(train_input_ids, train_labels)]\n",
        "valid_data_list = [{'valid_input_ids': input_ids, 'valid_labels': label} for input_ids, label in zip(valid_input_ids, valid_labels)]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IntentDataset(train_data_list, train_labels_encoded.tolist())\n",
        "valid_dataset = IntentDataset(valid_data_list, valid_labels_encoded.tolist())\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "\n",
        "# Calculate class weights for handling imbalanced classes\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Assuming you have a DataFrame 'data' with a column 'intent' that contains intent labels\n",
        "num_classes = len(data['intent'].unique())\n",
        "\n",
        "\n",
        "# Initialize the GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# Define the optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "# Define the loss function with class weights\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "total_valid_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in valid_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_valid_loss += loss.item()\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_gpt2_intent_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "k9xtz590ilFi",
        "outputId": "9f9385ee-a81c-4b63-e298-de372e312970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "Invalid element found in valid_data: 0.0\n",
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'input_ids'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-8f6af48f1218>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Initialize LabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Encode the non-numeric column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m             \u001b[0mcol_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0;31m# this is a cached value, mark it so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m     _index_shared_docs[\n\u001b[1;32m   3806\u001b[0m         \u001b[0;34m\"get_indexer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Data Cleaning\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join words back into text\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "data['utterance_cleaned'] = data['utterance'].apply(clean_text)\n",
        "\n",
        "# Text Tokenization\n",
        "data['tokens'] = data['utterance_cleaned'].apply(word_tokenize)\n",
        "\n",
        "# Linguistic Flags\n",
        "def adapt_response(text, flags):\n",
        "    # Example logic: Modify the text based on linguistic flags\n",
        "    if 'P' in flags:\n",
        "        text = \"Thank you for your question.\"\n",
        "    if 'Q' in flags:\n",
        "        text = \"Sure thing, ask away!\"\n",
        "    return text\n",
        "\n",
        "data['bot_response'] = data.apply(lambda row: adapt_response(row['utterance'], row['flags']), axis=1)\n",
        "\n",
        "# Save preprocessed data\n",
        "data.to_csv('preprocessed_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ASgHPpNLdyuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcDspcTDdcEo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join words back into text\n",
        "    return ' '.join(filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def clean_text(text):\n",
        "    # ... (previous cleaning steps)\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    # ... (rest of the cleaning steps)\n"
      ],
      "metadata": {
        "id": "sD1n8gFdd4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def synonym_replacement(tokens):\n",
        "    new_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stopwords.words('english'):\n",
        "            synonyms = wn.synsets(token)\n",
        "            if synonyms:\n",
        "                synonym = synonyms[0].lemmas()[0].name()\n",
        "                new_tokens.append(synonym)\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "        else:\n",
        "            new_tokens.append(token)\n",
        "    return new_tokens\n",
        "\n",
        "def adapt_response(text, flags):\n",
        "    if 'L' in flags:\n",
        "        tokens = word_tokenize(text)\n",
        "        new_tokens = synonym_replacement(tokens)\n",
        "        text = ' '.join(new_tokens)\n",
        "    # ... (other flag-based response adaptations)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "RXlXYSSsd85g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessed data as JSON\n",
        "data.to_json('preprocessed_dataset.json', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "na-H9WTueFtf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}